{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from function import fun1,fun2\n",
    "from my_NN import Mynetwork\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 确保 model 文件夹存在\n",
    "os.makedirs('./model_parameter', exist_ok=True)\n",
    "\n",
    "debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_t_per_interval_and_generate_f_start(dtype,device,seg_step=16,batch_size=32,f_start_min=0.1,f_start_max=0.9):\n",
    "    t_seg=torch.linspace(0,1,seg_step+1,dtype=dtype,device=device)\n",
    "    t_seg=t_seg.repeat(batch_size,1)\n",
    "    \n",
    "    rand_move=torch.empty((batch_size,seg_step+1),dtype=dtype,device=device).uniform_(-0.2/seg_step,0.2/seg_step)\n",
    "    rand_move[:,0]=0\n",
    "    rand_move[:,-1]=0\n",
    "    \n",
    "    # t_seg+=rand_move\n",
    "    # #t_seg.unsqueeze_(-1) #这里计划留到后面网络之前再加上处理\n",
    "    \n",
    "    f_start_rand=torch.empty((batch_size,1),dtype=dtype,device=device).uniform_(f_start_min,f_start_max)\n",
    "    \n",
    "    return t_seg , f_start_rand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 11])\n",
      "tensor([[0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "         0.9000, 1.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#function test\n",
    "k,k_f=sample_t_per_interval_and_generate_f_start(torch.float32,torch.device('cuda'),seg_step=10,batch_size=32)\n",
    "print(k.shape)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consist_loss_calculator(model,t_seg,f_start_rand,seg_step,loss_func,batch_size,boundary_func=fun1,depth=5,f_start_min=0.01,f_start_max=0.99):\n",
    "    #注意这里t_seg还是size为(batch_size,seg_step+1)的tensor\n",
    "    t_seg_copy=t_seg.clone()\n",
    "    dtype=t_seg.dtype\n",
    "    device=t_seg.device\n",
    "    consist_loss=0\n",
    "    for i in range(depth):\n",
    "        \n",
    "        mid_point=(t_seg_copy[:,1:]+t_seg_copy[:,:-1])/2\n",
    "        #这里构建下一个深度的t_seg_copy\n",
    "        temp_result=torch.empty((batch_size,t_seg_copy.shape[1]*2-1),dtype=t_seg.dtype,device=t_seg.device)\n",
    "        #不知道torch.empty的多次使用会不会导致内存消耗过大?\n",
    "        temp_result[:,::2]=t_seg_copy\n",
    "        temp_result[:,1::2]=mid_point\n",
    "        \n",
    "        #这里利用 mid_point 和 t_seg_copy 计算这一层的consist_loss\n",
    "        #至于f的初值为什么可以取随机数，还没想太明白\n",
    "        \n",
    "        delta=t_seg_copy[:,1:]-t_seg_copy[:,:-1]\n",
    "        \n",
    "        delta=delta.unsqueeze(-1)\n",
    "        #unsqueeze并不改变delta的维度，所以需要进行赋值，而delta.unsqueeze_(-1)可以直接改变delta的维度\n",
    "        \n",
    "        f_rand_n=torch.empty(delta.shape,dtype=dtype,device=device).uniform_(f_start_min,f_start_max)\n",
    "        \n",
    "        one_step_result=f_rand_n+delta*model.forward(f_rand_n,t_seg_copy[:,:-1].unsqueeze(-1),delta)\n",
    "        \n",
    "        two_step_result=f_rand_n+(delta/2)*model.forward(f_rand_n,t_seg_copy[:,:-1].unsqueeze(-1),delta/2)\n",
    "        two_step_result=two_step_result+(delta/2)*model.forward(two_step_result,mid_point.unsqueeze(-1),delta/2)\n",
    "        '''delta 忘乘了'''\n",
    "        \n",
    "        if debug:\n",
    "            print('depth:',i,'\\n')\n",
    "            print('one_step_result:',one_step_result.squeeze(-1),one_step_result.shape) \n",
    "            print('two_step_result:',two_step_result.squeeze(-1),two_step_result.shape)\n",
    "        \n",
    "        #归一化方案，这里的归一化方案是对每个batch的consist_loss进行归一化，而不是对整个batch的consist_loss进行归一化，采用了lixiang的归一化方案\n",
    "        norm_factors = torch.sum(delta**2, dim=1).mean()\n",
    "        if debug:\n",
    "            print('norm_factors:',norm_factors.squeeze(-1),norm_factors.shape)\n",
    "        \n",
    "        #consist_loss+=loss_func(one_step_result/norm_factors,two_step_result/norm_factors)\n",
    "    \n",
    "        \n",
    "        consist_loss+=loss_func(one_step_result,two_step_result)/depth/norm_factors\n",
    "        \n",
    "        t_seg_copy=temp_result\n",
    "        if debug:print('temp_result:',temp_result.shape)\n",
    "    return consist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_loss_calculator(model,t_seg,f_start_rand,seg_step,loss_func,batch_size,boundary_func=fun1,f_start_min=0.1,f_start_max=0.9):\n",
    "    #注意这里t_seg还是size为(batch_size,seg_step+1)的tensor\n",
    "    t_seg_copy=t_seg.clone()\n",
    "    t_seg_copy=t_seg_copy.unsqueeze_(-1)\n",
    "    delta=t_seg_copy[:,1:]-t_seg_copy[:,:-1]\n",
    "    if debug:\n",
    "        print('t_seg_copy:',t_seg_copy.squeeze(-1))\n",
    "    \n",
    "    func_f_end_value=boundary_func(f_start_rand)\n",
    "    \n",
    "    model_f_end_value=f_start_rand\n",
    "    for i in range(seg_step):\n",
    "        model_f_end_value=model_f_end_value + delta[:,i] * model.forward(model_f_end_value,t_seg_copy[:,i],delta[:,i])\n",
    "    '''这里delta也忘乘了'''\n",
    "    \n",
    "    # model_one_step=f_start_rand+(t_seg_copy[:,-1]-t_seg_copy[:,0])*model.forward(f_start_rand,t_seg_copy[:,0],t_seg_copy[:,-1]-t_seg_copy[:,0])\n",
    "    \n",
    "    #boundary_loss=loss_func(model_f_end_value,func_f_end_value)+loss_func(model_one_step,func_f_end_value)\n",
    "    boundary_loss=loss_func(model_f_end_value,func_f_end_value)\n",
    "    #是否需要做归一化？\n",
    "    \n",
    "    return boundary_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model,loss_func,batch_size,seg_step,dtype,device,boundary_func=fun1,f_start_min=0.1,f_start_max=0.6,consist_depth=5):\n",
    "    t_seg,f_start_rand=sample_t_per_interval_and_generate_f_start(dtype=dtype,device=device,seg_step=seg_step,batch_size=batch_size,f_start_min=f_start_min,f_start_max=f_start_max)\n",
    "    \n",
    "    boundary_loss=boundary_loss_calculator(model,t_seg,f_start_rand,seg_step,loss_func,batch_size,boundary_func=boundary_func,f_start_min=f_start_min,f_start_max=f_start_max)\n",
    "    \n",
    "    consist_loss=consist_loss_calculator(model,t_seg,f_start_rand,seg_step,loss_func,batch_size,boundary_func=boundary_func,f_start_min=f_start_min,f_start_max=f_start_max,depth=consist_depth)/(seg_step*(2**consist_depth))\n",
    "    \n",
    "    if debug:\n",
    "        print('boundary_loss:',boundary_loss)\n",
    "        print('consist_loss:',consist_loss)\n",
    "        \n",
    "    loss=boundary_loss+consist_loss\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(model,optimizer,loss_func,device,dtype,batch_size,consist_depth=5,boundary_func=fun1):\n",
    "    #train_loop\n",
    "    optimizer.zero_grad()\n",
    "    #计算loss\n",
    "    loss=get_loss(model,loss_func,batch_size,seg_step=16,dtype=dtype,device=device,boundary_func=boundary_func,consist_depth=consist_depth,f_start_min=0.1,f_start_max=0.6)\n",
    "    #反向传播\n",
    "    loss.backward()\n",
    "    #更新参数\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,device='cuda',dtype=torch.float32,\n",
    "          epoch=100,batch_size=32,lr=0.01,consist_depth=5,\n",
    "          use_lr_scheduler=False,boundary_func=fun1):\n",
    "    loss_list = []\n",
    "    \n",
    "    loss_func = nn.MSELoss()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 **({\"fused\": True} if \"cuda\" in str(device) else {}))#这里的fused=True，是为了使用apex加速\n",
    "\n",
    "    # Use ReduceLROnPlateau as the learning rate scheduler\n",
    "    if use_lr_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                               patience=60,\n",
    "                                                               threshold=1e-4,\n",
    "                                                               cooldown=3)\n",
    "    \n",
    "    init_time = time.time()\n",
    "    for i in range(epoch):\n",
    "        loss_term=iteration(model,optimizer,loss_func,device,dtype,batch_size,consist_depth=consist_depth,boundary_func=boundary_func)\n",
    "        #设计中，iteration中已经完成反向传播，所以这里不需要再进行反向传播\n",
    "        loss_list.append(loss_term)\n",
    "        if (i+1)%20==0:\n",
    "            print(f'epoch:{i+1},loss:{loss_term},time:{time.time()-init_time},lr:{optimizer.param_groups[0][\"lr\"]}')\n",
    "        if (i+1)%50==0:\n",
    "            torch.save(model.state_dict(),f'./model_parameter/model_para_batch{batch_size}_epoch{epoch}_consist_depth{consist_depth}.pth')\n",
    "        if use_lr_scheduler:\n",
    "            scheduler.step(loss_term)\n",
    "        if optimizer.param_groups[0][\"lr\"] <= 1.1e-8:\n",
    "            break\n",
    "    print('terminal epoch: ',i+1)\n",
    "    \n",
    "    if debug==False:\n",
    "        plt.plot(loss_list,label='loss')\n",
    "        plt.legend()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now using cuda !\n",
      "t_seg_copy: tensor([[0.0000, 0.0625, 0.1250,  ..., 0.8750, 0.9375, 1.0000],\n",
      "        [0.0000, 0.0625, 0.1250,  ..., 0.8750, 0.9375, 1.0000],\n",
      "        [0.0000, 0.0625, 0.1250,  ..., 0.8750, 0.9375, 1.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0625, 0.1250,  ..., 0.8750, 0.9375, 1.0000],\n",
      "        [0.0000, 0.0625, 0.1250,  ..., 0.8750, 0.9375, 1.0000],\n",
      "        [0.0000, 0.0625, 0.1250,  ..., 0.8750, 0.9375, 1.0000]],\n",
      "       device='cuda:0')\n",
      "depth: 0 \n",
      "\n",
      "one_step_result: tensor([[0.4222, 0.3230, 0.1414,  ..., 0.3919, 0.5250, 0.1674],\n",
      "        [0.2942, 0.5072, 0.4499,  ..., 0.1575, 0.3803, 0.1375],\n",
      "        [0.1371, 0.2569, 0.3504,  ..., 0.3403, 0.1094, 0.5339],\n",
      "        ...,\n",
      "        [0.5868, 0.5001, 0.2633,  ..., 0.2593, 0.5536, 0.4621],\n",
      "        [0.3513, 0.1903, 0.2325,  ..., 0.1645, 0.4336, 0.4252],\n",
      "        [0.3011, 0.5711, 0.2021,  ..., 0.4899, 0.3718, 0.4516]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 16, 1])\n",
      "two_step_result: tensor([[0.4222, 0.3230, 0.1414,  ..., 0.3919, 0.5250, 0.1674],\n",
      "        [0.2942, 0.5072, 0.4499,  ..., 0.1575, 0.3803, 0.1375],\n",
      "        [0.1371, 0.2569, 0.3503,  ..., 0.3403, 0.1094, 0.5339],\n",
      "        ...,\n",
      "        [0.5868, 0.5001, 0.2632,  ..., 0.2593, 0.5536, 0.4621],\n",
      "        [0.3513, 0.1903, 0.2325,  ..., 0.1645, 0.4336, 0.4251],\n",
      "        [0.3011, 0.5711, 0.2021,  ..., 0.4899, 0.3717, 0.4516]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 16, 1])\n",
      "norm_factors: tensor(0.0625, device='cuda:0') torch.Size([])\n",
      "temp_result: torch.Size([512, 33])\n",
      "depth: 1 \n",
      "\n",
      "one_step_result: tensor([[0.5042, 0.4430, 0.1240,  ..., 0.1506, 0.5451, 0.1394],\n",
      "        [0.4297, 0.5658, 0.3001,  ..., 0.1431, 0.5406, 0.1128],\n",
      "        [0.4627, 0.4318, 0.1761,  ..., 0.1644, 0.3866, 0.4556],\n",
      "        ...,\n",
      "        [0.2379, 0.5591, 0.3353,  ..., 0.3729, 0.1980, 0.5756],\n",
      "        [0.4801, 0.4573, 0.4741,  ..., 0.4724, 0.2581, 0.5537],\n",
      "        [0.2386, 0.3025, 0.5882,  ..., 0.5613, 0.2679, 0.1322]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 32, 1])\n",
      "two_step_result: tensor([[0.5042, 0.4430, 0.1240,  ..., 0.1506, 0.5451, 0.1394],\n",
      "        [0.4297, 0.5658, 0.3001,  ..., 0.1431, 0.5406, 0.1128],\n",
      "        [0.4627, 0.4318, 0.1761,  ..., 0.1644, 0.3866, 0.4556],\n",
      "        ...,\n",
      "        [0.2379, 0.5591, 0.3353,  ..., 0.3729, 0.1980, 0.5756],\n",
      "        [0.4801, 0.4573, 0.4741,  ..., 0.4724, 0.2581, 0.5537],\n",
      "        [0.2386, 0.3025, 0.5882,  ..., 0.5613, 0.2679, 0.1322]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 32, 1])\n",
      "norm_factors: tensor(0.0312, device='cuda:0') torch.Size([])\n",
      "temp_result: torch.Size([512, 65])\n",
      "depth: 2 \n",
      "\n",
      "one_step_result: tensor([[0.1813, 0.5181, 0.1849,  ..., 0.5250, 0.1523, 0.2495],\n",
      "        [0.2098, 0.3195, 0.2692,  ..., 0.2536, 0.4309, 0.2242],\n",
      "        [0.1660, 0.2401, 0.1874,  ..., 0.2407, 0.5942, 0.1691],\n",
      "        ...,\n",
      "        [0.4262, 0.5510, 0.1872,  ..., 0.3314, 0.2279, 0.2527],\n",
      "        [0.3802, 0.5424, 0.5964,  ..., 0.2310, 0.4247, 0.2578],\n",
      "        [0.3231, 0.4396, 0.1456,  ..., 0.3667, 0.5237, 0.2841]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 64, 1])\n",
      "two_step_result: tensor([[0.1813, 0.5181, 0.1849,  ..., 0.5250, 0.1523, 0.2495],\n",
      "        [0.2098, 0.3195, 0.2692,  ..., 0.2536, 0.4309, 0.2242],\n",
      "        [0.1660, 0.2401, 0.1874,  ..., 0.2407, 0.5942, 0.1691],\n",
      "        ...,\n",
      "        [0.4262, 0.5510, 0.1872,  ..., 0.3314, 0.2279, 0.2527],\n",
      "        [0.3802, 0.5424, 0.5964,  ..., 0.2309, 0.4247, 0.2578],\n",
      "        [0.3231, 0.4396, 0.1456,  ..., 0.3667, 0.5237, 0.2841]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 64, 1])\n",
      "norm_factors: tensor(0.0156, device='cuda:0') torch.Size([])\n",
      "temp_result: torch.Size([512, 129])\n",
      "depth: 3 \n",
      "\n",
      "one_step_result: tensor([[0.4934, 0.3936, 0.4145,  ..., 0.1982, 0.5474, 0.1419],\n",
      "        [0.2270, 0.1965, 0.1675,  ..., 0.2614, 0.1797, 0.2835],\n",
      "        [0.4549, 0.1460, 0.3943,  ..., 0.5644, 0.3345, 0.4838],\n",
      "        ...,\n",
      "        [0.3607, 0.2250, 0.5838,  ..., 0.3263, 0.4575, 0.5482],\n",
      "        [0.2151, 0.4245, 0.1758,  ..., 0.5374, 0.2286, 0.1412],\n",
      "        [0.1891, 0.3547, 0.4601,  ..., 0.3285, 0.1024, 0.1697]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 128, 1])\n",
      "two_step_result: tensor([[0.4934, 0.3936, 0.4145,  ..., 0.1982, 0.5474, 0.1419],\n",
      "        [0.2270, 0.1965, 0.1675,  ..., 0.2614, 0.1797, 0.2835],\n",
      "        [0.4549, 0.1460, 0.3943,  ..., 0.5644, 0.3345, 0.4838],\n",
      "        ...,\n",
      "        [0.3607, 0.2250, 0.5838,  ..., 0.3263, 0.4575, 0.5482],\n",
      "        [0.2151, 0.4245, 0.1758,  ..., 0.5374, 0.2286, 0.1412],\n",
      "        [0.1891, 0.3547, 0.4601,  ..., 0.3285, 0.1024, 0.1697]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 128, 1])\n",
      "norm_factors: tensor(0.0078, device='cuda:0') torch.Size([])\n",
      "temp_result: torch.Size([512, 257])\n",
      "depth: 4 \n",
      "\n",
      "one_step_result: tensor([[0.4882, 0.5744, 0.2961,  ..., 0.5103, 0.4425, 0.5792],\n",
      "        [0.5709, 0.4684, 0.3219,  ..., 0.2312, 0.5089, 0.3145],\n",
      "        [0.1411, 0.1364, 0.3745,  ..., 0.4851, 0.5643, 0.3261],\n",
      "        ...,\n",
      "        [0.1087, 0.2088, 0.4565,  ..., 0.5277, 0.5323, 0.3900],\n",
      "        [0.3712, 0.3200, 0.1296,  ..., 0.4689, 0.2951, 0.4099],\n",
      "        [0.5034, 0.1652, 0.3000,  ..., 0.1230, 0.5680, 0.5330]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 256, 1])\n",
      "two_step_result: tensor([[0.4882, 0.5744, 0.2961,  ..., 0.5103, 0.4425, 0.5792],\n",
      "        [0.5709, 0.4684, 0.3219,  ..., 0.2312, 0.5089, 0.3145],\n",
      "        [0.1411, 0.1364, 0.3745,  ..., 0.4851, 0.5643, 0.3261],\n",
      "        ...,\n",
      "        [0.1087, 0.2088, 0.4565,  ..., 0.5277, 0.5323, 0.3900],\n",
      "        [0.3712, 0.3200, 0.1296,  ..., 0.4689, 0.2951, 0.4099],\n",
      "        [0.5034, 0.1652, 0.3000,  ..., 0.1230, 0.5680, 0.5330]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([512, 256, 1])\n",
      "norm_factors: tensor(0.0039, device='cuda:0') torch.Size([])\n",
      "temp_result: torch.Size([512, 513])\n",
      "boundary_loss: tensor(0.1326, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "consist_loss: tensor(3.4770e-12, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "terminal epoch:  1\n",
      "program ended here \n",
      " terminal time:  1.0242810249328613\n",
      "save model parameter in file name : model_para_batch512_epoch1_consist_depth5.pth\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    initial_time=time.time()\n",
    "    if torch.cuda.is_available():\n",
    "        device='cuda'\n",
    "        print('now using cuda !')\n",
    "    else:\n",
    "        device='cpu'\n",
    "        print('now using cpu !')\n",
    "    dtype=torch.float32\n",
    "    model=Mynetwork().to(device=device,dtype=dtype)\n",
    "    \n",
    "    from torch.nn import init\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            init.normal_(param, mean=0, std=0.1)\n",
    "        elif 'bias' in name:\n",
    "            init.constant_(param, val=0)\n",
    "    \n",
    "    batch_size=512\n",
    "    epoch=10000\n",
    "    if debug:epoch=1\n",
    "    consist_depth=5\n",
    "    #training part \n",
    "    training(model,device,dtype,epoch=epoch,lr=0.001,batch_size=batch_size,consist_depth=consist_depth,use_lr_scheduler=True,boundary_func=fun1)\n",
    "    \n",
    "    print('program ended here \\n terminal time: ', time.time()-initial_time)\n",
    "    print(f'save model parameter in file name : model_para_batch{batch_size}_epoch{epoch}_consist_depth{consist_depth}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
