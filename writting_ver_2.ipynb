{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from function import fun1,fun2\n",
    "from my_NN import Mynetwork\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 确保 model 文件夹存在\n",
    "os.makedirs('./model_parameter', exist_ok=True)\n",
    "\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_t_per_interval_and_generate_f_start(dtype,device,seg_step=16,batch_size=32,f_start_min=0.1,f_start_max=0.9):\n",
    "    t_seg=torch.linspace(0,1,seg_step+1,dtype=dtype,device=device)\n",
    "    t_seg=t_seg.repeat(batch_size,1)\n",
    "    \n",
    "    rand_move=torch.empty((batch_size,seg_step+1),dtype=dtype,device=device).uniform_(-0.2/seg_step,0.2/seg_step)\n",
    "    rand_move[:,0]=0\n",
    "    rand_move[:,-1]=0\n",
    "    \n",
    "    t_seg+=rand_move\n",
    "    \n",
    "    f_start_rand=torch.empty((batch_size,1),dtype=dtype,device=device).uniform_(f_start_min,f_start_max)\n",
    "    \n",
    "    return t_seg , f_start_rand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consist_loss_calculator(model,t_seg,f_start_rand,seg_step,loss_func,batch_size,boundary_func=fun1,depth=5,f_start_min=0.01,f_start_max=0.99):\n",
    "    #注意这里t_seg还是size为(batch_size,seg_step+1)的tensor\n",
    "    t_seg_copy=t_seg.clone()\n",
    "    dtype=t_seg.dtype\n",
    "    device=t_seg.device\n",
    "    consist_loss=0\n",
    "    for i in range(depth):\n",
    "        \n",
    "        mid_point=(t_seg_copy[:,1:]+t_seg_copy[:,:-1])/2\n",
    "        #这里构建下一个深度的t_seg_copy\n",
    "        temp_result=torch.empty((batch_size,t_seg_copy.shape[1]*2-1),dtype=t_seg.dtype,device=t_seg.device)\n",
    "        #不知道torch.empty的多次使用会不会导致内存消耗过大?\n",
    "        temp_result[:,::2]=t_seg_copy\n",
    "        temp_result[:,1::2]=mid_point\n",
    "        \n",
    "        #这里利用 mid_point 和 t_seg_copy 计算这一层的consist_loss\n",
    "        #至于f的初值为什么可以取随机数，还没想太明白\n",
    "        \n",
    "        delta=t_seg_copy[:,1:]-t_seg_copy[:,:-1]\n",
    "        \n",
    "        delta=delta.unsqueeze(-1)\n",
    "        #unsqueeze并不改变delta的维度，所以需要进行赋值，而delta.unsqueeze_(-1)可以直接改变delta的维度\n",
    "        \n",
    "        f_rand_n=torch.empty(delta.shape,dtype=dtype,device=device).uniform_(f_start_min,f_start_max)\n",
    "        \n",
    "        one_step_result=f_rand_n+delta*model.forward(f_rand_n,t_seg_copy[:,:-1].unsqueeze(-1),delta)\n",
    "        \n",
    "        two_step_result=f_rand_n+(delta/2)*model.forward(f_rand_n,t_seg_copy[:,:-1].unsqueeze(-1),delta/2)\n",
    "        two_step_result=two_step_result+(delta/2)*model.forward(two_step_result,mid_point.unsqueeze(-1),delta/2)\n",
    "        '''delta 忘乘了'''\n",
    "        \n",
    "        if debug:\n",
    "            print('depth:',i,'\\n')\n",
    "            print('one_step_result:',one_step_result.squeeze(-1),one_step_result.shape) \n",
    "            print('two_step_result:',two_step_result.squeeze(-1),two_step_result.shape)\n",
    "        \n",
    "        #归一化方案，这里的归一化方案是对每个batch的consist_loss进行归一化，而不是对整个batch的consist_loss进行归一化，采用了lixiang的归一化方案\n",
    "        norm_factors = torch.sum(delta**2, dim=1).mean()\n",
    "        if debug:\n",
    "            print('norm_factors:',norm_factors.squeeze(-1),norm_factors.shape)\n",
    "        \n",
    "        #consist_loss+=loss_func(one_step_result/norm_factors,two_step_result/norm_factors)\n",
    "    \n",
    "        \n",
    "        consist_loss+=loss_func(one_step_result,two_step_result)/depth/norm_factors\n",
    "        \n",
    "        t_seg_copy=temp_result\n",
    "        if debug:print('temp_result:',temp_result.shape)\n",
    "    return consist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_loss_calculator(model,t_seg,f_start_rand,seg_step,loss_func,batch_size,boundary_func=fun1,f_start_min=0.1,f_start_max=0.9):\n",
    "    #注意这里t_seg还是size为(batch_size,seg_step+1)的tensor\n",
    "    t_seg_copy=t_seg.clone()\n",
    "    t_seg_copy=t_seg_copy.unsqueeze_(-1)\n",
    "    \n",
    "    log_step=int(np.log2(seg_step))\n",
    "    \n",
    "    delta=t_seg_copy[:,1:]-t_seg_copy[:,:-1]\n",
    "    func_f_end_value=boundary_func(f_start_rand)\n",
    "    \n",
    "    model_f_end_value=f_start_rand\n",
    "    for i in range(seg_step):\n",
    "        model_f_end_value=model_f_end_value + delta[:,i] * model.forward(model_f_end_value,t_seg_copy[:,i],delta[:,i])\n",
    "    \n",
    "    boundary_loss=loss_func(model_f_end_value,func_f_end_value)    \n",
    "    #####\n",
    "    temp_step=seg_step\n",
    "    \n",
    "    for i in range(log_step):\n",
    "        t_seg_copy=t_seg_copy[:,::2]\n",
    "        temp_step=temp_step//2\n",
    "        delta=t_seg_copy[:,1:]-t_seg_copy[:,:-1]\n",
    "        model_f_end_value=f_start_rand\n",
    "        for i in range(temp_step):\n",
    "            model_f_end_value=model_f_end_value + delta[:,i] * model.forward(model_f_end_value,t_seg_copy[:,i],delta[:,i])\n",
    "        boundary_loss+=loss_func(model_f_end_value,func_f_end_value)\n",
    "    \n",
    "    \n",
    "    return boundary_loss/log_step\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model,loss_func,batch_size,seg_step,dtype,device,boundary_func=fun1,f_start_min=0.1,f_start_max=0.6,consist_depth=5):\n",
    "    t_seg,f_start_rand=sample_t_per_interval_and_generate_f_start(dtype=dtype,device=device,seg_step=seg_step,batch_size=batch_size,f_start_min=f_start_min,f_start_max=f_start_max)\n",
    "    \n",
    "    boundary_loss=boundary_loss_calculator(model,t_seg,f_start_rand,seg_step,loss_func,batch_size,boundary_func=boundary_func,f_start_min=f_start_min,f_start_max=f_start_max)\n",
    "    \n",
    "    consist_loss=consist_loss_calculator(model,t_seg,f_start_rand,seg_step,loss_func,batch_size,boundary_func=boundary_func,f_start_min=f_start_min,f_start_max=f_start_max,depth=consist_depth)/(seg_step*(2**consist_depth))\n",
    "    \n",
    "    if debug:\n",
    "        print('boundary_loss:',boundary_loss)\n",
    "        print('consist_loss:',consist_loss)\n",
    "        \n",
    "    loss=boundary_loss+consist_loss\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(model,optimizer,loss_func,device,dtype,batch_size,consist_depth=5,boundary_func=fun1):\n",
    "    #train_loop\n",
    "    optimizer.zero_grad()\n",
    "    #计算loss\n",
    "    loss=get_loss(model,loss_func,batch_size,seg_step=16,dtype=dtype,device=device,boundary_func=boundary_func,consist_depth=consist_depth,f_start_min=0.1,f_start_max=0.6)\n",
    "    #反向传播\n",
    "    loss.backward()\n",
    "    #更新参数\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,device='cuda',dtype=torch.float32,\n",
    "          epoch=100,batch_size=32,lr=0.01,consist_depth=5,\n",
    "          use_lr_scheduler=False,boundary_func=fun1):\n",
    "    loss_list = []\n",
    "    \n",
    "    loss_func = nn.MSELoss()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 **({\"fused\": True} if \"cuda\" in str(device) else {}))#这里的fused=True，是为了使用apex加速\n",
    "\n",
    "    # Use ReduceLROnPlateau as the learning rate scheduler\n",
    "    if use_lr_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                               patience=60,\n",
    "                                                               threshold=1e-4,\n",
    "                                                               cooldown=3)\n",
    "    \n",
    "    init_time = time.time()\n",
    "    for i in range(epoch):\n",
    "        loss_term=iteration(model,optimizer,loss_func,device,dtype,batch_size,consist_depth=consist_depth,boundary_func=boundary_func)\n",
    "        #设计中，iteration中已经完成反向传播，所以这里不需要再进行反向传播\n",
    "        loss_list.append(loss_term)\n",
    "        if (i+1)%20==0:\n",
    "            print(f'epoch:{i+1},loss:{loss_term},time:{time.time()-init_time},lr:{optimizer.param_groups[0][\"lr\"]}')\n",
    "        if (i+1)%50==0:\n",
    "            torch.save(model.state_dict(),f'./model_parameter/model_para_batch{batch_size}_epoch{epoch}_consist_depth{consist_depth}.pth')\n",
    "        if use_lr_scheduler:\n",
    "            scheduler.step(loss_term)\n",
    "        if optimizer.param_groups[0][\"lr\"] <= 1.1e-8:\n",
    "            break\n",
    "    print('terminal epoch: ',i+1)\n",
    "    \n",
    "    if debug==False:\n",
    "        plt.plot(loss_list,label='loss')\n",
    "        plt.legend()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now using cuda !\n",
      "epoch:20,loss:0.09211017191410065,time:0.7367486953735352,lr:0.001\n",
      "epoch:40,loss:0.05725131928920746,time:1.460970401763916,lr:0.001\n",
      "epoch:60,loss:0.05173809826374054,time:2.1753482818603516,lr:0.001\n",
      "epoch:80,loss:0.038812004029750824,time:2.890026807785034,lr:0.001\n",
      "epoch:100,loss:0.017688987776637077,time:3.588210105895996,lr:0.001\n",
      "epoch:120,loss:0.006006293464452028,time:4.308679580688477,lr:0.001\n",
      "epoch:140,loss:0.004551934543997049,time:5.0019142627716064,lr:0.001\n",
      "epoch:160,loss:0.0030734373722225428,time:5.697753190994263,lr:0.001\n",
      "epoch:180,loss:0.0021568145602941513,time:6.410452842712402,lr:0.001\n",
      "epoch:200,loss:0.0017008672002702951,time:7.017308712005615,lr:0.001\n",
      "epoch:220,loss:0.0013896457385271788,time:7.6537158489227295,lr:0.001\n",
      "epoch:240,loss:0.0012222066288813949,time:8.294682502746582,lr:0.001\n",
      "epoch:260,loss:0.0008495711372233927,time:8.981876134872437,lr:0.001\n",
      "epoch:280,loss:0.0007334521505981684,time:9.589167594909668,lr:0.001\n",
      "epoch:300,loss:0.0006790122133679688,time:10.202113628387451,lr:0.001\n",
      "epoch:320,loss:0.0006319716339930892,time:10.841877460479736,lr:0.001\n",
      "epoch:340,loss:0.00047313713002949953,time:11.489763498306274,lr:0.001\n",
      "epoch:360,loss:0.0005071772611699998,time:12.11765718460083,lr:0.001\n",
      "epoch:380,loss:0.00045270321425050497,time:12.74574613571167,lr:0.001\n",
      "epoch:400,loss:0.0004044674278702587,time:13.385684251785278,lr:0.001\n",
      "epoch:420,loss:0.00036026007728651166,time:14.022484540939331,lr:0.001\n",
      "epoch:440,loss:0.0003399471170268953,time:14.657385349273682,lr:0.001\n",
      "epoch:460,loss:0.0002634745615068823,time:15.314736604690552,lr:0.001\n",
      "epoch:480,loss:0.0002574862737674266,time:15.956026077270508,lr:0.001\n",
      "epoch:500,loss:0.00023592381330672652,time:16.65267300605774,lr:0.001\n",
      "epoch:520,loss:0.00024762804969213903,time:17.37835717201233,lr:0.001\n",
      "epoch:540,loss:0.00018310142331756651,time:18.07694911956787,lr:0.001\n",
      "epoch:560,loss:0.00017550641496200114,time:18.771247625350952,lr:0.001\n",
      "epoch:580,loss:0.00021203464712016284,time:19.495230197906494,lr:0.001\n",
      "epoch:600,loss:0.00017483142437413335,time:20.197399854660034,lr:0.001\n",
      "epoch:620,loss:0.00015826475282665342,time:20.920811414718628,lr:0.001\n",
      "epoch:640,loss:0.00016751716611906886,time:21.63607096672058,lr:0.001\n",
      "epoch:660,loss:0.0001227477187057957,time:22.33265495300293,lr:0.001\n",
      "epoch:680,loss:0.00012879565474577248,time:23.03232169151306,lr:0.001\n",
      "epoch:700,loss:0.00011966424062848091,time:23.737125873565674,lr:0.001\n",
      "epoch:720,loss:0.0001158790837507695,time:24.463183879852295,lr:0.001\n",
      "epoch:740,loss:0.0001075878826668486,time:25.188169479370117,lr:0.001\n",
      "epoch:760,loss:0.00010820455645443872,time:25.913235187530518,lr:0.001\n",
      "epoch:780,loss:8.152836380759254e-05,time:26.640494346618652,lr:0.001\n",
      "epoch:800,loss:9.189579577650875e-05,time:27.35668683052063,lr:0.001\n",
      "epoch:820,loss:8.63129025674425e-05,time:28.065268516540527,lr:0.001\n",
      "epoch:840,loss:7.952281885081902e-05,time:28.781545639038086,lr:0.001\n",
      "epoch:860,loss:0.00010033047874458134,time:29.489787101745605,lr:0.001\n",
      "epoch:880,loss:9.063754259841517e-05,time:30.196739673614502,lr:0.001\n",
      "epoch:900,loss:7.14402922312729e-05,time:30.926878452301025,lr:0.001\n",
      "epoch:920,loss:6.606762326555327e-05,time:31.6485276222229,lr:0.001\n",
      "epoch:940,loss:6.414227391360328e-05,time:32.36172342300415,lr:0.001\n",
      "epoch:960,loss:7.032762368908152e-05,time:33.086297273635864,lr:0.001\n",
      "epoch:980,loss:5.116054308018647e-05,time:33.7909152507782,lr:0.001\n",
      "epoch:1000,loss:6.193751323735341e-05,time:34.50304317474365,lr:0.001\n",
      "epoch:1020,loss:6.778416718589142e-05,time:35.195117712020874,lr:0.0001\n",
      "epoch:1040,loss:6.270490121096373e-05,time:35.894898653030396,lr:0.0001\n",
      "epoch:1060,loss:5.775933823315427e-05,time:36.62143421173096,lr:0.0001\n",
      "epoch:1080,loss:5.302103454596363e-05,time:37.324788093566895,lr:0.0001\n",
      "epoch:1100,loss:5.695913569070399e-05,time:38.04478883743286,lr:0.0001\n",
      "epoch:1120,loss:5.881975084776059e-05,time:38.781875133514404,lr:0.0001\n",
      "epoch:1140,loss:6.37108605587855e-05,time:39.49689793586731,lr:1e-05\n",
      "epoch:1160,loss:7.118644862202927e-05,time:40.194610834121704,lr:1e-05\n",
      "epoch:1180,loss:4.860296758124605e-05,time:40.883936643600464,lr:1e-05\n",
      "epoch:1200,loss:6.133564602350816e-05,time:41.53134083747864,lr:1e-05\n",
      "epoch:1220,loss:6.057830250938423e-05,time:42.179871559143066,lr:1e-05\n",
      "epoch:1240,loss:5.68592113268096e-05,time:42.83191108703613,lr:1e-05\n",
      "epoch:1260,loss:5.848233195138164e-05,time:43.47957372665405,lr:1.0000000000000002e-06\n",
      "epoch:1280,loss:5.219712693360634e-05,time:44.11783003807068,lr:1.0000000000000002e-06\n",
      "epoch:1300,loss:5.822431921842508e-05,time:44.7488431930542,lr:1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    initial_time=time.time()\n",
    "    if torch.cuda.is_available():\n",
    "        device='cuda'\n",
    "        print('now using cuda !')\n",
    "    else:\n",
    "        device='cpu'\n",
    "        print('now using cpu !')\n",
    "    dtype=torch.float32\n",
    "    model=Mynetwork().to(device=device,dtype=dtype)\n",
    "    \n",
    "    from torch.nn import init\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            init.normal_(param, mean=0, std=0.1)\n",
    "        elif 'bias' in name:\n",
    "            init.constant_(param, val=0)\n",
    "    \n",
    "    batch_size=512\n",
    "    epoch=10000\n",
    "    if debug:epoch=1\n",
    "    consist_depth=5\n",
    "    #training part \n",
    "    training(model,device,dtype,epoch=epoch,lr=0.001,batch_size=batch_size,consist_depth=consist_depth,use_lr_scheduler=True,boundary_func=fun1)\n",
    "    \n",
    "    print('program ended here \\n terminal time: ', time.time()-initial_time)\n",
    "    print(f'save model parameter in file name : model_para_batch{batch_size}_epoch{epoch}_consist_depth{consist_depth}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
